{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d5e4375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e00af57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model:int, vocab_size:int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea380862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[ 1, 15, 16,  7, 14],\n",
      "        [ 0, 19, 15,  3, 16]])\n",
      "\n",
      "Embeddings (with scaling):\n",
      " tensor([[[ 0.6206, -4.8051,  3.7036, -4.6989,  4.9056,  1.8863, -2.9454,\n",
      "          -3.9649],\n",
      "         [ 6.2646, -0.3866, -2.8798,  0.5046,  3.6842,  3.0988, -2.4335,\n",
      "           1.8151],\n",
      "         [-0.2685,  3.1137,  3.7065, -0.8283,  1.1825, -3.2015, -2.3173,\n",
      "          -2.6892],\n",
      "         [ 1.5138,  5.5813, -0.5869, -0.0865,  0.7345,  1.7297,  3.3416,\n",
      "          -3.1249],\n",
      "         [-0.7383,  2.2570, -3.1315,  6.5919, -2.9574,  1.9825,  5.9677,\n",
      "          -3.1789]],\n",
      "\n",
      "        [[-0.4645, -2.7477, -2.9157,  1.8308, -4.1248, -0.7967, -2.9132,\n",
      "          -1.9471],\n",
      "         [-0.8472,  6.4950,  0.9349,  6.1512, -0.6388, -0.6811,  4.8512,\n",
      "          -2.1354],\n",
      "         [ 6.2646, -0.3866, -2.8798,  0.5046,  3.6842,  3.0988, -2.4335,\n",
      "           1.8151],\n",
      "         [ 2.0912,  3.5446, -1.2573,  2.3149,  0.0353,  2.7596, -0.0228,\n",
      "          -0.1128],\n",
      "         [-0.2685,  3.1137,  3.7065, -0.8283,  1.1825, -3.2015, -2.3173,\n",
      "          -2.6892]]], grad_fn=<MulBackward0>)\n",
      "\n",
      "Norm of each embedding vector (after scaling):\n",
      " tensor([[10.5493,  8.9630,  6.9600,  7.6326, 10.8303],\n",
      "        [ 7.0388, 10.5157,  8.9630,  5.6131,  6.9600]],\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d_model = 8\n",
    "vocab_size = 20\n",
    "batch_size, seq_len = 2, 5\n",
    "\n",
    "layer = InputEmbeddings(d_model, vocab_size)\n",
    "\n",
    "# Some random token IDs\n",
    "tokens = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "print(\"Token IDs:\\n\", tokens)\n",
    "\n",
    "# Raw embeddings (with scaling)\n",
    "out = layer(tokens)\n",
    "print(\"\\nEmbeddings (with scaling):\\n\", out)\n",
    "\n",
    "# To see their size, print the L2 norm of each token vector\n",
    "norms = out.norm(dim=-1)\n",
    "print(\"\\nNorm of each embedding vector (after scaling):\\n\", norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22f36b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodings(nn.Module):\n",
    "    def __init__(self, d_model:int, seq_len:int, dropout:float) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model \n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) \n",
    "        pe[:, 1::2] = torch.cos(position * div_term) \n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9028681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape (before PE): torch.Size([2, 5, 8])\n",
      "out shape (after PE): torch.Size([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "pos_enc = PositionalEncodings(d_model=8, seq_len=10, dropout=0.1)\n",
    "\n",
    "# 2. Example: a batch of 2 sentences, each with 5 tokens, already embedded\n",
    "batch_size = 2\n",
    "sentence_len = 5\n",
    "x = torch.randn(batch_size, sentence_len, 8)   # random \"word embeddings\"\n",
    "\n",
    "print(\"x shape (before PE):\", x.shape)\n",
    "\n",
    "# 3. Pass through positional encoding\n",
    "out = pos_enc(x)\n",
    "print(\"out shape (after PE):\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc6858ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps:float=10**-6) ->None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(-1,keepdim=True)\n",
    "        std = x.std(-1,keepdim=True)\n",
    "        return self.alpha * (x- mean)/(std+ self.eps) +self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21cd4ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [10., 20., 30., 40.]])\n",
      "\n",
      "Output after LayerNorm:\n",
      " tensor([[-0.1619,  0.6127,  1.3873,  2.1619],\n",
      "        [-0.1619,  0.6127,  1.3873,  2.1619]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Means per token after LN: tensor([1., 1.], grad_fn=<MeanBackward1>)\n",
      "Stds per token after LN : tensor([1.0000, 1.0000], grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [1.0, 2.0, 3.0, 4.0],   # token 1\n",
    "    [10.0, 20.0, 30.0, 40.0] # token 2\n",
    "])  # shape (2, 4)  â†’ 2 tokens, each with 4 features\n",
    "\n",
    "ln = LayerNormalization()\n",
    "y = ln(x)\n",
    "\n",
    "print(\"Input:\\n\", x)\n",
    "print(\"\\nOutput after LayerNorm:\\n\", y)\n",
    "\n",
    "print(\"\\nMeans per token after LN:\", y.mean(-1))\n",
    "print(\"Stds per token after LN :\", y.std(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b4b49a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model:int, d_ff:int, dropout:float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(d_model,d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff,d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.linear_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "91ec4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model:int, d_ff:int,dropout:float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(d_model,d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff,d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.linear_2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a28844b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForwardBlock(d_model,4,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d111d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1754, -0.3231, -0.0633, -0.2627, -0.1509,  0.1274, -0.2511,  0.1314],\n",
      "        [ 0.3001,  0.0232, -0.2356, -0.1267,  0.0772, -0.2695,  0.1756, -0.3210],\n",
      "        [-0.3399, -0.3435, -0.0717,  0.2377, -0.3346,  0.2939, -0.1414,  0.1036],\n",
      "        [ 0.0161, -0.3188,  0.2932,  0.1904,  0.3514,  0.1786, -0.2334,  0.2951]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0190,  0.1677, -0.2835, -0.1017], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4909, -0.1947,  0.1079, -0.3926],\n",
      "        [ 0.1594,  0.2684,  0.0697, -0.3345],\n",
      "        [-0.3877, -0.1543,  0.2195,  0.4932],\n",
      "        [ 0.2875, -0.0563,  0.1753, -0.4905],\n",
      "        [-0.4271,  0.2333, -0.2832,  0.2405],\n",
      "        [-0.3530, -0.2477, -0.4118,  0.2609],\n",
      "        [-0.0509,  0.3848,  0.3094,  0.2767],\n",
      "        [ 0.0161, -0.1546, -0.1087,  0.0665]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2479, -0.3503,  0.4196, -0.0544, -0.4190, -0.2705,  0.4424,  0.4573],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in ff.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "02ccb987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "torch.Size([4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(ff.linear_1.weight.shape)\n",
    "print(ff.linear_1.bias.shape)\n",
    "print(ff.linear_2.weight.shape)\n",
    "print(ff.linear_2.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1fcce9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1754, -0.3231, -0.0633, -0.2627, -0.1509,  0.1274, -0.2511,  0.1314],\n",
      "        [ 0.3001,  0.0232, -0.2356, -0.1267,  0.0772, -0.2695,  0.1756, -0.3210],\n",
      "        [-0.3399, -0.3435, -0.0717,  0.2377, -0.3346,  0.2939, -0.1414,  0.1036],\n",
      "        [ 0.0161, -0.3188,  0.2932,  0.1904,  0.3514,  0.1786, -0.2334,  0.2951]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0190,  0.1677, -0.2835, -0.1017], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4909, -0.1947,  0.1079, -0.3926],\n",
      "        [ 0.1594,  0.2684,  0.0697, -0.3345],\n",
      "        [-0.3877, -0.1543,  0.2195,  0.4932],\n",
      "        [ 0.2875, -0.0563,  0.1753, -0.4905],\n",
      "        [-0.4271,  0.2333, -0.2832,  0.2405],\n",
      "        [-0.3530, -0.2477, -0.4118,  0.2609],\n",
      "        [-0.0509,  0.3848,  0.3094,  0.2767],\n",
      "        [ 0.0161, -0.1546, -0.1087,  0.0665]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2479, -0.3503,  0.4196, -0.0544, -0.4190, -0.2705,  0.4424,  0.4573],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(ff.linear_1.weight)\n",
    "print(ff.linear_1.bias)\n",
    "print(ff.linear_2.weight)\n",
    "print(ff.linear_2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fbe9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model:int, h:int , dropout:float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        assert d_model // h == 0, 'd_model is not divisible by h'\n",
    "\n",
    "        self.d_k = self.d_model // h\n",
    "        self.w_q = nn.Linear(d_model,d_model)\n",
    "        self.w_v = nn.Linear(d_model,d_model)\n",
    "        self.w_k = nn.Linear(d_model,d_model)\n",
    "\n",
    "        self.w_o = nn.Linear(d_model,d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k=query.shape[-1]\n",
    "\n",
    "        attention_score = (query @ key.transpose(-1,-2))/math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_score.masked_fill(mask == 0 , -1e9)\n",
    "        attention_score = attention_score.softmax(dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_score = dropout(attention_score)\n",
    "        \n",
    "        return (attention_score @value) , attention_score\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, q,k,v, mask):\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        query = query.vew(query.shape[0], query.hsape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1 ,2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1 ,2)\n",
    "\n",
    "        x, self.attention_score = self.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        x = x.transpose(1,2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        return self.w_o(x)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
